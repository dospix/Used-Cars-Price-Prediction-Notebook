{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3e7e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ad298",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv(\"./used_cars.csv\")\n",
    "cars = cars.rename(columns={\n",
    "    \"year\": \"entry_year\",\n",
    "    \"title_status\": \"vehicle_status\",\n",
    "    \"size\": \"vehicle_size\",\n",
    "    \"type\": \"vehicle_type\"\n",
    "})\n",
    "cars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac55520",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "### Removing duplicates and irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d23fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length before removing duplicates: {len(cars)}\")\n",
    "\n",
    "clean_cars = cars.drop([\"id\", \"region\", \"VIN\", \"county\", \"lat\", \"long\", \"posting_date\"], axis=1)\n",
    "# These columns are removed because they were found to have no correlation in a future heatmap\n",
    "clean_cars = clean_cars.drop([\"paint_color\", \"state\"], axis=1)\n",
    "clean_cars.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "print(f\"Length after removing duplicates: {len(clean_cars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ab5b1b",
   "metadata": {},
   "source": [
    "### Dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40acb17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_null_values_count_per_column(dataframe):\n",
    "    end_output = \"\"\n",
    "    for column in dataframe.columns:\n",
    "        end_output += f\"nulls in {column}: {len(dataframe[dataframe[column].isnull()])},\\n\"\n",
    "    end_output = end_output.rstrip(\",\\n\")\n",
    "    print(end_output)\n",
    "\n",
    "print_null_values_count_per_column(clean_cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length before removing null values: {len(clean_cars)}\")\n",
    "\n",
    "# Car model is essential for predicting price, thus null values are dropped\n",
    "clean_cars = clean_cars.dropna(subset=\"model\")\n",
    "\n",
    "# year and odometer nulls are difficult to fill, since there are few of them they will be dropped\n",
    "clean_cars = clean_cars.dropna(subset=[\"entry_year\", \"odometer\"])\n",
    "\n",
    "# for columns with few null values, merge them in the most common category\n",
    "# otherwise place them in their own \"unknown\" group\n",
    "clean_cars.manufacturer = clean_cars.manufacturer.fillna(\"unknown\")\n",
    "clean_cars.condition = clean_cars.condition.fillna(\"unknown\")\n",
    "clean_cars.cylinders = clean_cars.cylinders.fillna(\"unknown\")\n",
    "clean_cars.fuel = clean_cars.fuel.fillna(\"gas\")\n",
    "clean_cars.vehicle_status = clean_cars.vehicle_status.fillna(\"clean\")\n",
    "clean_cars.transmission = clean_cars.transmission.fillna(\"automatic\")\n",
    "clean_cars.drive = clean_cars.drive.fillna(\"unknown\")\n",
    "clean_cars.vehicle_size = clean_cars.vehicle_size.fillna(\"unknown\")\n",
    "clean_cars.vehicle_type = clean_cars.vehicle_type.fillna(\"unknown\")\n",
    "\n",
    "print(f\"Length after removing null values: {len(clean_cars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d885e865",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_null_values_count_per_column(clean_cars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fc0973",
   "metadata": {},
   "source": [
    "### Changing string columns to numerical columns where possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daf56ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_cars.condition = clean_cars.condition.map({\n",
    "    \"unknown\": -1,\n",
    "    \"salvage\": 0,\n",
    "    \"fair\": 1,\n",
    "    \"good\": 2,\n",
    "    \"excellent\": 3,\n",
    "    \"like new\": 4,\n",
    "    \"new\": 5\n",
    "})\n",
    "clean_cars.cylinders = clean_cars.cylinders.map({\n",
    "    \"unknown\": -1,\n",
    "    \"other\": 0,\n",
    "    \"3 cylinders\": 3,\n",
    "    \"4 cylinders\": 4,\n",
    "    \"5 cylinders\": 5,\n",
    "    \"6 cylinders\": 6,\n",
    "    \"8 cylinders\": 8,\n",
    "    \"10 cylinders\": 10,\n",
    "    \"12 cylinders\": 12\n",
    "})\n",
    "clean_cars.vehicle_size = clean_cars.vehicle_size.map({\n",
    "    \"unknown\": -1,\n",
    "    \"sub-compact\": 0,\n",
    "    \"compact\": 1,\n",
    "    \"mid-size\": 2,\n",
    "    \"full-size\": 3\n",
    "})\n",
    "\n",
    "clean_cars.price = clean_cars.price.astype(int)\n",
    "clean_cars.entry_year = clean_cars.entry_year.astype(int)\n",
    "clean_cars.odometer = clean_cars.odometer.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aff062",
   "metadata": {},
   "source": [
    "## Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a07e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep all prices under 1M$ because big prices mess with the histogram below\n",
    "no_outliers = clean_cars.copy()\n",
    "no_outliers.price = no_outliers.price[no_outliers.price < 1000000]\n",
    "no_outliers.price = no_outliers.price[no_outliers.price >= 1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8d08c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a histogram of every column that could have outliers to see which ones have outliers\n",
    "# Alongside there will be plotted 2 vertical lines representing the bounds for eliminating outliers\n",
    "columns_used_for_checking_outliers = [\"price\", \"entry_year\", \"odometer\"]\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "fig.subplots_adjust(hspace=0.9, wspace=0.2)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for subplot_index, column_name in enumerate(columns_used_for_checking_outliers):\n",
    "    ax = axes[subplot_index]\n",
    "    ax.hist(no_outliers[column_name], bins=75, rwidth=0.8)\n",
    "    \n",
    "    mean = no_outliers[column_name].mean()\n",
    "    standard_deviation = no_outliers[column_name].std()\n",
    "    \n",
    "    lower_bound = mean - (3 * standard_deviation)\n",
    "    upper_bound = mean + (3 * standard_deviation)\n",
    "\n",
    "    ax.axvline(x=lower_bound, color='b')\n",
    "    ax.axvline(x=upper_bound, color='b')\n",
    "    \n",
    "    ax.set_xlabel(column_name)\n",
    "    ax.set_ylabel(\"frequency\")\n",
    "    ax.set_title(f\"Distribution of {column_name}\")\n",
    "    if column_name != \"entry_year\":\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.set_title(f\"Distribution of {column_name} (logarithmic scale)\")\n",
    "\n",
    "plt.ticklabel_format(style='plain', axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98025b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length before removing outliers: {len(clean_cars)}\\n\")\n",
    "\n",
    "columns_used_for_removing_outliers = [\"price\", \"entry_year\", \"odometer\"]\n",
    "\n",
    "for column_name in columns_used_for_removing_outliers:\n",
    "    mean = no_outliers[column_name].mean()\n",
    "    standard_deviation = no_outliers[column_name].std()\n",
    "    \n",
    "    lower_bound = mean - (3 * standard_deviation)\n",
    "    upper_bound = mean + (3 * standard_deviation)\n",
    "    \n",
    "    percentage_removed = round((((no_outliers[column_name] < lower_bound) | (no_outliers[column_name] > upper_bound)).sum() / len(no_outliers)) * 100, 2)\n",
    "\n",
    "    print(f\"For column {column_name}, removing a percentage of {percentage_removed}% values.\")\n",
    "    no_outliers = no_outliers[(lower_bound <= no_outliers[column_name]) & (no_outliers[column_name] <= upper_bound)]\n",
    "\n",
    "print(f\"\\nLength after removing outliers: {len(no_outliers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d3bbc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns_used_for_checking_outliers = [\"price\", \"entry_year\", \"odometer\"]\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "fig.subplots_adjust(hspace=0.9, wspace=0.2)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for subplot_index, column_name in enumerate(columns_used_for_checking_outliers):\n",
    "    ax = axes[subplot_index]\n",
    "    ax.hist(no_outliers[column_name], bins=25, rwidth=0.8)\n",
    "    \n",
    "    ax.set_xlabel(column_name)\n",
    "    ax.set_ylabel(\"frequency\")\n",
    "    ax.set_title(f\"Distribution of {column_name}\")\n",
    "\n",
    "plt.ticklabel_format(style='plain', axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db16fa5",
   "metadata": {},
   "source": [
    "### Erasing models that don't appear often #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82f7fef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Length before removing models that don't appear often: {len(no_outliers)}\\n\")\n",
    "\n",
    "# This operation is done 2 times, normally it is done after we've completely cleaned the dataset to remove very rare models\n",
    "# However the cells below takes a lot of time to complete, so it is first done here to remove some of the cars, then again later\n",
    "model_counts = no_outliers.model.value_counts()\n",
    "values_to_keep = model_counts[model_counts >= 10].index\n",
    "no_outliers = no_outliers[no_outliers.model.isin(values_to_keep)]\n",
    "\n",
    "print(f\"Length after removing models that don't appear often: {len(no_outliers)}\\n\")\n",
    "no_outliers.model.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57db367a",
   "metadata": {},
   "source": [
    "### Erase price outliers for each car model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a9a65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length before removing outliers for each car model: {len(no_outliers)}\\n\")\n",
    "\n",
    "car_model_groups = no_outliers.groupby('model')\n",
    "\n",
    "def remove_outliers(group):\n",
    "    price_mean = group['price'].mean()\n",
    "    price_std = group['price'].std()\n",
    "\n",
    "    lower_bound = price_mean - (2 * price_std)\n",
    "    upper_bound = price_mean + (2 * price_std)\n",
    "\n",
    "    lower_outliers_mask = group['price'] >= lower_bound\n",
    "    upper_outliers_mask = group['price'] <= upper_bound\n",
    "\n",
    "    return group[lower_outliers_mask & upper_outliers_mask]\n",
    "\n",
    "no_outliers = car_model_groups.apply(remove_outliers)\n",
    "no_outliers.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Length after removing outliers for each car model: {len(no_outliers)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27555aac",
   "metadata": {},
   "source": [
    "### Eliminating better cars that are cheaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba86a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length before removing better cars that are cheaper: {len(no_outliers)}\\n\")\n",
    "final_df = no_outliers.copy()\n",
    "\n",
    "# These columns are used because a car with condition 0 would normally be considered better than a car with condition -1\n",
    "# But condition -1 (unknown) is as good if not usually better than 0 (salvage)\n",
    "# Once all better cars that are cheaper are erased, these columns will be dropped to prevent multicollinearity problems\n",
    "final_df[\"condition_unknown\"] = np.where(final_df[\"condition\"] == -1, 1, 0)\n",
    "final_df[\"cylinders_unknown\"] = np.where(final_df[\"cylinders\"] == -1, 1, 0)\n",
    "final_df[\"vehicle_size_unknown\"] = np.where(final_df[\"vehicle_size\"] == -1, 1, 0)\n",
    "\n",
    "for model_index, model_name in tqdm(enumerate(final_df.model.unique()), total=len(final_df.model.unique())):\n",
    "    curr_cars = final_df[final_df.model == model_name].sort_values(by='price', ascending=False)\n",
    "    to_be_deleted = set()\n",
    "    for car_index, car in curr_cars.iterrows():\n",
    "        if car_index in to_be_deleted:\n",
    "            continue\n",
    "        better_cheaper_cars = curr_cars[\n",
    "            (curr_cars.manufacturer == car.manufacturer) &\n",
    "            (curr_cars.entry_year >= car.entry_year) &\n",
    "            (curr_cars.condition >= car.condition) &\n",
    "            (curr_cars.condition_unknown == car.condition_unknown) &\n",
    "            (curr_cars.cylinders >= car.cylinders) &\n",
    "            (curr_cars.cylinders_unknown == car.cylinders_unknown) &\n",
    "            (curr_cars.fuel == car.fuel) &\n",
    "            (curr_cars.odometer <= car.odometer) &\n",
    "            (curr_cars.vehicle_status == car.vehicle_status) &\n",
    "            (curr_cars.transmission == car.transmission) &\n",
    "            (curr_cars.drive == car.drive) &\n",
    "            (curr_cars.vehicle_size >= car.vehicle_size) &\n",
    "            (curr_cars.vehicle_size_unknown == car.vehicle_size_unknown) &\n",
    "            (curr_cars.vehicle_type == car.vehicle_type) &\n",
    "            (curr_cars.price <= car.price) &\n",
    "            (curr_cars.index != car_index)\n",
    "        ]\n",
    "        \n",
    "        if len(better_cheaper_cars):\n",
    "            to_be_deleted.update(better_cheaper_cars.index)\n",
    "    final_df = final_df.drop(to_be_deleted)\n",
    "\n",
    "final_df = final_df.drop([\"condition_unknown\", \"cylinders_unknown\", \"vehicle_size_unknown\"], axis=1)\n",
    "print(f\"\\nLength after removing better cars that are cheaper: {len(final_df)}\")\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c11d33",
   "metadata": {},
   "source": [
    "### Erasing models that don't appear often #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc2f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length before removing models that don't appear often: {len(final_df)}\\n\")\n",
    "\n",
    "model_counts = final_df.model.value_counts()\n",
    "values_to_keep = model_counts[model_counts >= 10].index\n",
    "final_df = final_df[final_df.model.isin(values_to_keep)]\n",
    "\n",
    "print(f\"Length after removing models that don't appear often: {len(final_df)}\\n\")\n",
    "final_df.model.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6aae81",
   "metadata": {},
   "source": [
    "### Correlation heatmap, used to remove irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e115f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "heatmap_df = final_df.copy()\n",
    "\n",
    "# Label encode string columns\n",
    "label_encoder = LabelEncoder()\n",
    "string_columns = ['manufacturer', 'model', \"condition\", \"cylinders\", 'fuel', 'vehicle_status', 'transmission', 'drive', \"vehicle_size\", 'vehicle_type']\n",
    "for col in string_columns:\n",
    "    heatmap_df[col] = label_encoder.fit_transform(heatmap_df[col])\n",
    "\n",
    "correlation_matrix = heatmap_df.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(correlation_matrix, cmap='coolwarm', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(correlation_matrix.shape[0]):\n",
    "    for j in range(correlation_matrix.shape[1]):\n",
    "        plt.text(j, i, f\"{correlation_matrix.iloc[i, j]:.2f}\", ha='center', va='center', color='white', fontsize=8)\n",
    "\n",
    "plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)\n",
    "plt.yticks(range(len(correlation_matrix.index)), correlation_matrix.index)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955c7a2f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a3b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_encode = [\"manufacturer\", \"model\", \"fuel\", \"vehicle_status\", \"transmission\", \"drive\", \"vehicle_type\"]\n",
    "\n",
    "final_df = pd.get_dummies(final_df, columns=columns_to_encode, prefix=columns_to_encode, drop_first=True)\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958e91b1",
   "metadata": {},
   "source": [
    "## First phase hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0381ab",
   "metadata": {},
   "source": [
    "### Test on 1000 samples and pick the top 100 configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508311f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_phase_df = final_df.head(1000)\n",
    "X = first_phase_df.drop(\"price\", axis=1)\n",
    "y = first_phase_df.price\n",
    "\n",
    "models = { \n",
    "    \"linear_regression\": {\n",
    "        \"steps\": [\n",
    "            (\"scaler\", MinMaxScaler()),\n",
    "            (\"regressor\", linear_model.LinearRegression())\n",
    "        ],\n",
    "        \"params\": {}\n",
    "    },\n",
    "    \"knn_regression\": {\n",
    "        \"steps\": [\n",
    "            (\"scaler\", MinMaxScaler()),\n",
    "            (\"regressor\", KNeighborsRegressor())\n",
    "        ],\n",
    "        \"params\": {\n",
    "            \"regressor__n_neighbors\": [5, 10, 25],\n",
    "            \"regressor__weights\": [\"uniform\", \"distance\"],\n",
    "            \"regressor__algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\"],\n",
    "            \"regressor__leaf_size\": [20, 30, 40],\n",
    "            \"regressor__p\": [1, 2, 3]\n",
    "        }\n",
    "    },\n",
    "    \"suppor_vector_regression\": {\n",
    "        \"steps\": [\n",
    "            (\"scaler\", MinMaxScaler()),\n",
    "            (\"regressor\", SVR())\n",
    "        ],\n",
    "        \"params\": {\n",
    "            \"regressor__kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "            \"regressor__C\": [0.1, 1, 10, 100],\n",
    "            \"regressor__epsilon\": [0.1, 0.01, 0.001],\n",
    "            \"regressor__gamma\": [\"scale\", \"auto\", 0.1, 1],\n",
    "            \"regressor__degree\": [2, 3]\n",
    "        }\n",
    "    },\n",
    "    \"random_forest_regression\": {\n",
    "        \"steps\": [\n",
    "            (\"scaler\", MinMaxScaler()),\n",
    "            (\"regressor\", RandomForestRegressor())\n",
    "        ],\n",
    "        \"params\": {\n",
    "            \"regressor__n_estimators\": [50, 100, 250, 500],\n",
    "            \"regressor__max_depth\": [None, 5, 10, 20],\n",
    "            \"regressor__min_samples_split\": [2, 5, 10],\n",
    "            \"regressor__min_samples_leaf\": [1, 2, 5],\n",
    "            \"regressor__max_features\": [1.0, 'sqrt', 'log2']\n",
    "        }\n",
    "    },\n",
    "    \"gradient_boosting_regression\": {\n",
    "        \"steps\": [\n",
    "            (\"scaler\", MinMaxScaler()),\n",
    "            (\"regressor\", GradientBoostingRegressor())\n",
    "        ],\n",
    "        \"params\": {\n",
    "            \"regressor__learning_rate\": [0.1, 0.01, 0.001],\n",
    "            \"regressor__n_estimators\": [50, 100, 250, 500],\n",
    "            \"regressor__max_depth\": [None, 5, 10, 20],\n",
    "            \"regressor__min_samples_split\": [2, 5, 10],\n",
    "            \"regressor__min_samples_leaf\": [1, 2, 5],\n",
    "            \"regressor__max_features\": [1.0, 'sqrt', 'log2']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, options in tqdm(models.items(), desc=\"Hyperparameter Tuning\"):\n",
    "    pipeline = Pipeline(options[\"steps\"])\n",
    "    grid_search = GridSearchCV(pipeline, options[\"params\"], cv=5, return_train_score=False, verbose = 4)\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    score_results = grid_search.cv_results_['mean_test_score']\n",
    "    params_results = grid_search.cv_results_['params']\n",
    "    \n",
    "    for score, params in zip(score_results, params_results):\n",
    "        scores.append({\n",
    "            'model': model_name,\n",
    "            'score': score,\n",
    "            'params': params\n",
    "        })\n",
    "\n",
    "scores_df = pd.DataFrame(scores, columns=['model', 'score', 'params'])\n",
    "scores_df = scores_df.sort_values('score', ascending=False)\n",
    "scores_df.reset_index(drop=True, inplace=True)\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fb5e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596c0fe",
   "metadata": {},
   "source": [
    "## Second phase hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7161eb42",
   "metadata": {},
   "source": [
    "### Test winners from phase 1 on 10000 samples and pick the top 5 configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d87aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_phase_df = final_df.head(10000)\n",
    "X = second_phase_df.drop(\"price\", axis=1)\n",
    "y = second_phase_df.price\n",
    "\n",
    "models = { \n",
    "    \"gradient_boosting_regression\": {\n",
    "        \"steps\": [\n",
    "            (\"scaler\", MinMaxScaler()),\n",
    "            (\"regressor\", GradientBoostingRegressor())\n",
    "        ],\n",
    "        \"params\": {\n",
    "            \"regressor__learning_rate\": [0.1],\n",
    "            \"regressor__n_estimators\": [100, 250, 500],\n",
    "            \"regressor__max_depth\": [None, 5, 10, 20],\n",
    "            \"regressor__min_samples_split\": [2, 5, 10],\n",
    "            \"regressor__min_samples_leaf\": [1, 2, 5],\n",
    "            \"regressor__max_features\": ['sqrt', 'log2']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, options in tqdm(models.items(), desc=\"Hyperparameter Tuning\"):\n",
    "    pipeline = Pipeline(options[\"steps\"])\n",
    "    grid_search = GridSearchCV(pipeline, options[\"params\"], cv=5, return_train_score=False, verbose = 4)\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    score_results = grid_search.cv_results_['mean_test_score']\n",
    "    params_results = grid_search.cv_results_['params']\n",
    "    \n",
    "    for score, params in zip(score_results, params_results):\n",
    "        scores.append({\n",
    "            'model': model_name,\n",
    "            'score': score,\n",
    "            'params': params\n",
    "        })\n",
    "\n",
    "scores_df = pd.DataFrame(scores, columns=['model', 'score', 'params'])\n",
    "scores_df = scores_df.sort_values('score', ascending=False)\n",
    "scores_df.reset_index(drop=True, inplace=True)\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab2f207",
   "metadata": {},
   "source": [
    "## Final phase hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a37adb",
   "metadata": {},
   "source": [
    "### Test winners from phase 2 on all samples and pick the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb32959",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_phase_df = final_df.copy()\n",
    "X = final_phase_df.drop(\"price\", axis=1)\n",
    "y = final_phase_df.price\n",
    "\n",
    "models = { \n",
    "    \"gradient_boosting_regression\": {\n",
    "        \"steps\": [\n",
    "            (\"scaler\", MinMaxScaler()),\n",
    "            (\"regressor\", GradientBoostingRegressor())\n",
    "        ],\n",
    "        \"params\": {\n",
    "            \"regressor__learning_rate\": [0.1],\n",
    "            \"regressor__n_estimators\": [500],\n",
    "            \"regressor__max_depth\": [None],\n",
    "            \"regressor__min_samples_split\": [2, 10],\n",
    "            \"regressor__min_samples_leaf\": [2, 5],\n",
    "            \"regressor__max_features\": ['sqrt']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, options in tqdm(models.items(), desc=\"Hyperparameter Tuning\"):\n",
    "    pipeline = Pipeline(options[\"steps\"])\n",
    "    grid_search = GridSearchCV(pipeline, options[\"params\"], cv=3, return_train_score=False, verbose = 4)\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    score_results = grid_search.cv_results_['mean_test_score']\n",
    "    params_results = grid_search.cv_results_['params']\n",
    "    \n",
    "    for score, params in zip(score_results, params_results):\n",
    "        scores.append({\n",
    "            'model': model_name,\n",
    "            'score': score,\n",
    "            'params': params\n",
    "        })\n",
    "\n",
    "scores_df = pd.DataFrame(scores, columns=['model', 'score', 'params'])\n",
    "scores_df = scores_df.sort_values('score', ascending=False)\n",
    "scores_df.reset_index(drop=True, inplace=True)\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ba19c",
   "metadata": {},
   "source": [
    "### Training final model and calculating score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c81813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df.drop(\"price\", axis=1)\n",
    "y = final_df.price\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Linear Regression model\n",
    "model = GradientBoostingRegressor(learning_rate=0.1, max_depth=None, max_features=\"sqrt\", min_samples_leaf=2, min_samples_split=10, n_estimators=500)\n",
    "\n",
    "# Train the model using the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "score = model.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ff6875",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the residuals\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Plot the residuals\n",
    "plt.scatter(y_pred, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d946fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4329af59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residuals = pd.DataFrame({'residuals': residuals})  # Create a DataFrame with residuals\n",
    "df_inputs = X_test.reset_index(drop=True)  # Reset the index of X_test DataFrame\n",
    "\n",
    "# Find rows with residuals greater than 10000\n",
    "outliers_dataframe = df_residuals[abs(df_residuals['residuals']) > 10000]\n",
    "\n",
    "# Get the corresponding inputs for outliers\n",
    "outliers_inputs = outliers_dataframe.loc[outliers_dataframe.index]\n",
    "outliers_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e778a93d",
   "metadata": {},
   "source": [
    "## Testing neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b5c057",
   "metadata": {},
   "source": [
    "### Using optuna to pick the best neural network for our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29e341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCHSIZE = 128\n",
    "\n",
    "X = final_df.drop(\"price\", axis=1)\n",
    "y = final_df.price\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values.reshape(-1, 1), dtype=torch.float32).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "valid_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "def define_model(trial):\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 4)\n",
    "    layers = []\n",
    "\n",
    "    in_features = len(X.columns)\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 4, 128)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = trial.suggest_float(\"dropout_l{}\".format(i), 0.2, 0.5)\n",
    "        layers.append(nn.Dropout(p))\n",
    "\n",
    "        in_features = out_features\n",
    "    \n",
    "    layers.append(nn.Linear(in_features, 1))\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Generate the model.\n",
    "    model = define_model(trial).to(DEVICE)\n",
    "\n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-1, log=True)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "\n",
    "    # Get the number of epochs as a hyperparameter from Optuna.\n",
    "    EPOCHS = trial.suggest_int(\"epochs\", 20, 200)\n",
    "\n",
    "    # Training of the model.\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.mse_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation of the model.\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "                data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)\n",
    "                output = model(data)\n",
    "                loss = F.mse_loss(output, target)  # Use MSE for evaluation\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        mse = total_loss / min(len(valid_loader.dataset), N_VALID_EXAMPLES)\n",
    "\n",
    "        trial.report(mse, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return mse\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78136969",
   "metadata": {},
   "source": [
    "### Testing the best neural network and comparing it with the best ml model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d92bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = 125\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "X = final_df.drop(\"price\", axis=1)\n",
    "y = final_df.price\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values.reshape(-1, 1), dtype=torch.float32).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "input_size = len(X.columns)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.42),\n",
    "    nn.Linear(128, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.245),\n",
    "    nn.Linear(50, 1)\n",
    ").to(device)\n",
    "\n",
    "# Define the loss function (Mean Squared Error for regression)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer (Adam)\n",
    "learning_rate = 0.0048\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    ### Training\n",
    "    model.train()\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Move batch data to device (GPU if available)\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(batch_X)\n",
    "  \n",
    "        # 2. Calculate loss\n",
    "        loss = loss_fn(y_pred, batch_y)\n",
    "        r2 = r2_score(batch_y.cpu().detach().numpy(), y_pred.cpu().detach().numpy())\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 1. Forward pass\n",
    "        test_pred = model(X_test_tensor)\n",
    "        # 2. Calculate test loss\n",
    "        test_loss = loss_fn(test_pred, y_test_tensor)\n",
    "        test_r2 = r2_score(y_test_tensor.cpu().detach().numpy(), test_pred.cpu().detach().numpy())\n",
    "\n",
    "    # Print out what's happening every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.1f} | R2: {r2:.4f} | Test loss: {test_loss:.1f} | Test R2: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39117367",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_pred = model(X_test_tensor).cpu().detach().numpy()\n",
    "\n",
    "    # Calculate the residuals\n",
    "    residuals = (y_test_tensor.cpu().detach().numpy() - test_pred).flatten()\n",
    "\n",
    "    # Plot the residuals\n",
    "    plt.scatter(test_pred, residuals)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel(\"Predicted values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(\"Residual Plot\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb982bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residuals = pd.DataFrame({'residuals': residuals})  # Create a DataFrame with residuals\n",
    "df_inputs = X_test.reset_index(drop=True)  # Reset the index of X_test DataFrame\n",
    "\n",
    "# Find rows with residuals greater than 10000\n",
    "outliers_dataframe = df_residuals[abs(df_residuals['residuals']) > 10000]\n",
    "\n",
    "# Get the corresponding inputs for outliers\n",
    "outliers_inputs = outliers_dataframe.loc[outliers_dataframe.index]\n",
    "outliers_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb32b4",
   "metadata": {},
   "source": [
    "### Exporting the neural network and the scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2173236",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"used_cars_price_prediction_neural_network.pt\")\n",
    "joblib.dump(scaler, \"used_cars_scaler.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
